{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10034748,"sourceType":"datasetVersion","datasetId":6180692},{"sourceId":180845,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":154112,"modelId":176589}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets\nfrom datasets import load_dataset, Dataset\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\nimport torch\nimport string\nimport re\nimport nltk\nimport pandas as pd\nimport json\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T17:02:47.019672Z","iopub.execute_input":"2024-11-27T17:02:47.019984Z","iopub.status.idle":"2024-11-27T17:03:15.797208Z","shell.execute_reply.started":"2024-11-27T17:02:47.019960Z","shell.execute_reply":"2024-11-27T17:03:15.796335Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import random\n# Load the dataset\ndataset = load_dataset(\"Jinyan1/COLING_2025_MGT_en\")\nfrom datasets import Dataset\n\ndef sample_fixed_balanced_dataset(dataset, total_samples=300000, target_label_col=\"label\"):\n    train_data = dataset['train']\n    label_to_samples = {}\n\n    # Group samples by label\n    for example in train_data:\n        label = int(example[target_label_col])\n        if label not in label_to_samples:\n            label_to_samples[label] = []\n        label_to_samples[label].append(example)\n\n    # Determine number of samples per class\n    samples_per_class = total_samples // len(label_to_samples)\n\n    # Sample from each class\n    balanced_data = []\n    for label, samples in label_to_samples.items():\n        if len(samples) < samples_per_class:\n            raise ValueError(f\"Not enough examples for label {label}. Reduce total_samples.\")\n        balanced_data.extend(random.sample(samples, samples_per_class))\n\n    # Shuffle and return as a new Dataset\n    return Dataset.from_list(balanced_data).shuffle(seed=42)\n\n# Subset the training dataset\ntrain_data = sample_fixed_balanced_dataset(dataset=dataset)\n\ndef preprocess_text(text):\n    \"\"\"Clean text by removing mentions, links, Unicode, and extra spaces.\"\"\"\n    mentions_pattern = re.compile(r'(@.*?)[\\s]')\n    links_pattern = re.compile(r'https?:\\/\\/[^\\s\\n\\r]+')\n    multi_spaces_pattern = re.compile(r'\\s+')\n\n    text = mentions_pattern.sub(' ', text)\n    text = links_pattern.sub(' ', text)\n    text = ''.join(char for char in text if ord(char) < 128)  # Remove Unicode\n    text = multi_spaces_pattern.sub(' ', text).strip()\n    return text\n\ndef preprocess_dataset(dataset):\n    \"\"\"Apply text preprocessing to a dataset.\"\"\"\n    return dataset.map(lambda example: {\"text\": preprocess_text(example[\"text\"])})\n\n# Load only the first 1000 rows for training\ntrain_data = preprocess_dataset(train_data)\ndev_data = preprocess_dataset(dataset['dev'])\n\n# Load RoBERTa tokenizer and define tokenization\ntokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n\n# Tokenize the datasets\ntrain_data = train_data.map(tokenize_function, batched=True)\ndev_data = dev_data.map(tokenize_function, batched=True)\n\n# Set format for PyTorch\ntrain_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\ndev_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T17:03:15.798761Z","iopub.execute_input":"2024-11-27T17:03:15.799332Z","iopub.status.idle":"2024-11-27T17:22:12.595354Z","shell.execute_reply.started":"2024-11-27T17:03:15.799268Z","shell.execute_reply":"2024-11-27T17:22:12.594416Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/588 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"840890a31bb44220b73f3abdea0895b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00002.parquet:   0%|          | 0.00/287M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"070f80d8fd9d4ad6a45f4427754ff362"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00002.parquet:   0%|          | 0.00/286M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5209784454a94f6fbe3d5cc1a6fa01a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dev-00000-of-00001.parquet:   0%|          | 0.00/246M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4d7168d4a764ecb96ba2995d18a6fbb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/610767 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5934b2753c7410aafe62bff31a5245f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating dev split:   0%|          | 0/261758 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1498f90e0444434a8e04a09d55eb1b63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/300000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50586c6acf1d4ec8b3aad70da6d2c2e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/261758 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aeddce46990c49f5a6da025d0f4d5fcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a54eae532ad42108a15a254c2214ab2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57fe1829a63f4de99697679c2ca0bb43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29b54efe147f4846a25c1aafb5c12559"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61668647ef0b4496a298816c1f64fa60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"502d6b17fb3e49ed805dca77d58dd180"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/300000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eff7dfe2125f4ab38acb64caf089ecf9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/261758 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c705f15354c9436486870bcdcd4d5122"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T17:22:12.596339Z","iopub.execute_input":"2024-11-27T17:22:12.596596Z","iopub.status.idle":"2024-11-27T17:22:12.602771Z","shell.execute_reply.started":"2024-11-27T17:22:12.596571Z","shell.execute_reply":"2024-11-27T17:22:12.601731Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'source', 'sub_source', 'lang', 'model', 'label', 'text'],\n        num_rows: 610767\n    })\n    dev: Dataset({\n        features: ['id', 'source', 'sub_source', 'lang', 'model', 'label', 'text'],\n        num_rows: 261758\n    })\n})"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"import torch.nn as nn\nfrom transformers import RobertaModel, RobertaPreTrainedModel\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n\n\nclass CustomRobertaForSequenceClassification(RobertaPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.roberta = RobertaModel(config)\n        self.dropout = nn.Dropout(0.3)  # Dropout for regularization\n        self.classifier = nn.Sequential(\n            nn.Linear(config.hidden_size, 256),  # Add a hidden layer\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, config.num_labels)  # Output layer\n        )\n        self.init_weights()\n\n    def forward(self, input_ids=None, attention_mask=None, labels=None):\n        # Get outputs from RoBERTa\n        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.pooler_output  # Use pooled output\n        pooled_output = self.dropout(pooled_output)  # Apply dropout\n        logits = self.classifier(pooled_output)\n\n        loss = None\n        if labels is not None:\n            loss_fn = nn.CrossEntropyLoss()\n            loss = loss_fn(logits, labels)\n\n        return {\"loss\": loss, \"logits\": logits}\n\n\n# Register the custom class with the pre-trained weights\nfrom transformers import RobertaConfig\nconfig = RobertaConfig.from_pretrained(\"roberta-base\", num_labels=2)\nmodel = CustomRobertaForSequenceClassification.from_pretrained(\"roberta-base\", config=config)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    report_to=\"none\",\n    save_total_limit=2,\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_data,\n    eval_dataset=dev_data,\n)\n\n# Train the model\ntrainer.train(resume_from_checkpoint=\"/kaggle/input/18500v2/transformers/default/1/checkpoint-18500\")\n\n# Evaluate on dev set\ndev_results = trainer.evaluate()\nprint(\"Dev Results:\", dev_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T17:22:12.825234Z","iopub.execute_input":"2024-11-27T17:22:12.825568Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65fcb3ba619e4863af87e4ead506b24c"}},"metadata":{}},{"name":"stderr","text":"Some weights of CustomRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.0.bias', 'classifier.0.weight', 'classifier.3.bias', 'classifier.3.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3262: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2944: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint_rng_state = torch.load(rng_file)\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='19804' max='56250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [19804/56250 1:31:28 < 42:40:44, 0.24 it/s, Epoch 1.06/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.139800</td>\n      <td>0.259685</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, recall_score\nimport torch\nimport json\nfrom transformers import RobertaTokenizer\nfrom torch.utils.data import DataLoader, TensorDataset\nimport re\nfrom transformers import DistilBertTokenizer, DistilBertConfig, Trainer, TrainingArguments\n\n# Load test data from JSONL\ndef load_test_data_from_jsonl(jsonl_file, tokenizer, max_length=512):\n    texts = []\n    labels = []\n    with open(jsonl_file, \"r\") as f:\n        for line in f:\n            item = json.loads(line.strip())\n            texts.append(preprocess_text(item[\"text\"]))\n            labels.append(item[\"label\"])\n    \n    tokenized_data = tokenizer(\n        texts,\n        truncation=True,\n        padding=\"max_length\",\n        max_length=max_length,\n        return_tensors=\"pt\"\n    )\n    dataset = TensorDataset(\n        tokenized_data[\"input_ids\"], \n        tokenized_data[\"attention_mask\"], \n        torch.tensor(labels)\n    )\n    return dataset\n\n# Load tokenizer\ntokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n\n# Path to test JSONL file\ntest_jsonl_path = \"/kaggle/input/nlp-roberta-test/test_set_en_with_label (1).jsonl\"\ntest_dataset = load_test_data_from_jsonl(test_jsonl_path, tokenizer)\n\n\n# Load the model\nfrom transformers import RobertaConfig\n\nconfig = RobertaConfig.from_pretrained(\"/kaggle/input/20000v2/transformers/default/1/checkpoint-20000\")\nmodel = CustomRobertaForSequenceClassification.from_pretrained(\"/kaggle/input/20000v2/transformers/default/1/checkpoint-20000\", config=config)\n\n# Evaluate the model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()\n\n# Create DataLoader\ntest_loader = DataLoader(test_dataset, batch_size=8)\n\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids, attention_mask, labels = batch\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n        labels = labels.to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs[\"logits\"]\n        preds = torch.argmax(logits, dim=-1)\n\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n# Calculate metrics\nprecision = precision_score(all_labels, all_preds)\nrecall = recall_score(all_labels, all_preds)\naccuracy = accuracy_score(all_labels, all_preds)\nmacro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\nmicro_f1 = f1_score(all_labels, all_preds, average=\"micro\")\nconf_matrix = confusion_matrix(all_labels, all_preds)\n\n# Print results\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"Macro F1 Score: {macro_f1:.4f}\")\nprint(f\"Micro F1 Score: {micro_f1:.4f}\")\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T18:34:04.847338Z","iopub.execute_input":"2024-11-28T18:34:04.847719Z","iopub.status.idle":"2024-11-28T19:13:06.764545Z","shell.execute_reply.started":"2024-11-28T18:34:04.847687Z","shell.execute_reply":"2024-11-28T19:13:06.763460Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.7229\nPrecision: 0.6703\nRecall: 0.9410\nMacro F1 Score: 0.7000\nMicro F1 Score: 0.7229\nConfusion Matrix:\n[[16503 18172]\n [ 2315 36951]]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}